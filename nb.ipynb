{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6e908cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# toy dataset\n",
    "train_texts = [\n",
    "    \"win a free vacation now\",          # spam\n",
    "    \"free lottery winner claim\",        # spam\n",
    "    \"call mom for dinner\",              # ham\n",
    "    \"schedule a meeting for tomorrow\",  # ham\n",
    "    \"winner winner free prize\",         # spam\n",
    "    \"are we still on for lunch\",        # ham\n",
    "]\n",
    "train_labels = [\"spam\", \"spam\", \"ham\", \"ham\", \"spam\", \"ham\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7a889109",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def tokenize(text):\n",
    "    # keep only letters and spaces, then split\n",
    "    tokens = re.findall(r\"[a-zA-Z]+\", text.lower())\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "13c4b762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['winner', 'winner', 'free', 'pri', 'e']\n"
     ]
    }
   ],
   "source": [
    "print(tokenize(\"winner! winner free pri$e\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ab75b2f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size: 21\n",
      "sample mapping: {'win': 0, 'a': 1, 'free': 2, 'vacation': 3, 'now': 4, 'lottery': 5, 'winner': 6, 'claim': 7, 'call': 8, 'mom': 9}\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def build_vocab(texts):\n",
    "    vocab = {}\n",
    "    for text in texts:\n",
    "        for tok in tokenize(text):\n",
    "            if tok not in vocab:\n",
    "                #print(len(vocab))\n",
    "                vocab[tok] = len(vocab)\n",
    "    return vocab\n",
    "\n",
    "vocab = build_vocab(train_texts)\n",
    "print(\"vocab size:\", len(vocab))\n",
    "print(\"sample mapping:\", dict(list(vocab.items())[:10]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bb685659",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['win a free vacation now',\n",
       " 'free lottery winner claim',\n",
       " 'call mom for dinner',\n",
       " 'schedule a meeting for tomorrow',\n",
       " 'winner winner free prize',\n",
       " 'are we still on for lunch']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4a305077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first example bow: [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "label: spam\n",
      "first example bow: [0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "label: spam\n"
     ]
    }
   ],
   "source": [
    "def vectorize(text, vocab):\n",
    "    vec = [0] * len(vocab)\n",
    "    for tok in tokenize(text):\n",
    "        if tok in vocab:\n",
    "            vec[vocab[tok]] += 1\n",
    "    return vec\n",
    "\n",
    "X = [vectorize(t, vocab) for t in train_texts]\n",
    "y = train_labels\n",
    "\n",
    "print(\"first example bow:\", X[0])\n",
    "print(\"label:\", y[0])\n",
    "\n",
    "print(\"first example bow:\", X[1])\n",
    "print(\"label:\", y[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d239949b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['win a free vacation now',\n",
       " 'free lottery winner claim',\n",
       " 'call mom for dinner',\n",
       " 'schedule a meeting for tomorrow',\n",
       " 'winner winner free prize',\n",
       " 'are we still on for lunch']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "56b6e95a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens across all texts: 28\n",
      "All tokens: ['win', 'a', 'free', 'vacation', 'now', 'free', 'lottery', 'winner', 'claim', 'call', 'mom', 'for', 'dinner', 'schedule', 'a', 'meeting', 'for', 'tomorrow', 'winner', 'winner', 'free', 'prize', 'are', 'we', 'still', 'on', 'for', 'lunch']\n"
     ]
    }
   ],
   "source": [
    "# tokenize each text in the list and count total tokens\n",
    "all_tokens = []\n",
    "for text in train_texts:\n",
    "    tokens = tokenize(text)\n",
    "    all_tokens.extend(tokens)\n",
    "\n",
    "print(\"Total tokens across all texts:\", len(all_tokens))\n",
    "print(\"All tokens:\", all_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3b307298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ham', 'spam'}\n",
      "{'ham': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'spam': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "class_counts: {'spam': 3, 'ham': 3}\n",
      "spam word total: 13\n",
      "ham word total: 15\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def count_words_by_class(texts, labels, vocab):\n",
    "    # word_counts[class][word] = count\n",
    "    print(set(labels))\n",
    "    word_counts = {c: [0]*len(vocab) for c in set(labels)}\n",
    "    print(word_counts)\n",
    "    class_counts = defaultdict(int)\n",
    "\n",
    "    for text, label in zip(texts, labels):\n",
    "        counts = vectorize(text, vocab)\n",
    "        #print(counts)\n",
    "        for i in range(len(vocab)):\n",
    "            word_counts[label][i] += counts[i]\n",
    "\n",
    "        #word_counts[label] = [wc + c for wc, c in zip(word_counts[label], counts)]\n",
    "        class_counts[label] += 1\n",
    "\n",
    "    return word_counts, class_counts\n",
    "\n",
    "word_counts, class_counts = count_words_by_class(train_texts, train_labels, vocab)\n",
    "\n",
    "print(\"class_counts:\", dict(class_counts))\n",
    "print(\"spam word total:\", sum(word_counts[\"spam\"]))\n",
    "print(\"ham word total:\", sum(word_counts[\"ham\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "00b9f8aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Understanding the element-wise addition:\n",
      "==================================================\n",
      "Existing counts: [1, 0, 2, 0, 1]\n",
      "New counts:      [0, 1, 1, 0, 2]\n",
      "\n",
      "zip() pairs elements:\n",
      "Position 0: existing=1, new=0, sum=1\n",
      "Position 1: existing=0, new=1, sum=1\n",
      "Position 2: existing=2, new=1, sum=3\n",
      "Position 3: existing=0, new=0, sum=0\n",
      "Position 4: existing=1, new=2, sum=3\n",
      "\n",
      "Result after addition: [1, 1, 3, 0, 3]\n",
      "\n",
      "==================================================\n",
      "In the context of our spam/ham classifier:\n",
      "- word_counts[label] = running total of word counts for that class\n",
      "- counts = word counts for the current document being processed\n",
      "- We're accumulating word frequencies across all documents in each class\n",
      "\n",
      "Our vocabulary has 21 words:\n",
      "Sample vocab mapping: {'win': 0, 'a': 1, 'free': 2, 'vacation': 3, 'now': 4}\n",
      "\n",
      "word_counts['spam'] has 21 elements (one per vocab word)\n",
      "First 10 spam word counts: [1, 1, 3, 1, 1, 1, 3, 1, 0, 0]\n",
      "First 10 ham word counts: [0, 1, 0, 0, 0, 0, 0, 0, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "# Let's break down this line step by step:\n",
    "# word_counts[label] = [wc + c for wc, c in zip(word_counts[label], counts)]\n",
    "\n",
    "print(\"Understanding the element-wise addition:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Example with simple numbers\n",
    "existing_counts = [1, 0, 2, 0, 1]  # Current word counts for a class\n",
    "new_counts =      [0, 1, 1, 0, 2]  # New document's word counts\n",
    "\n",
    "print(\"Existing counts:\", existing_counts)\n",
    "print(\"New counts:     \", new_counts)\n",
    "\n",
    "# The zip() function pairs up elements from both lists\n",
    "print(\"\\nzip() pairs elements:\")\n",
    "for i, (wc, c) in enumerate(zip(existing_counts, new_counts)):\n",
    "    print(f\"Position {i}: existing={wc}, new={c}, sum={wc + c}\")\n",
    "\n",
    "# List comprehension does the addition\n",
    "result = [wc + c for wc, c in zip(existing_counts, new_counts)]\n",
    "print(\"\\nResult after addition:\", result)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"In the context of our spam/ham classifier:\")\n",
    "print(\"- word_counts[label] = running total of word counts for that class\")\n",
    "print(\"- counts = word counts for the current document being processed\")\n",
    "print(\"- We're accumulating word frequencies across all documents in each class\")\n",
    "\n",
    "# Let's see this with actual vocab\n",
    "print(f\"\\nOur vocabulary has {len(vocab)} words:\")\n",
    "print(\"Sample vocab mapping:\", dict(list(vocab.items())[:5]))\n",
    "\n",
    "# Show actual word_counts structure\n",
    "print(f\"\\nword_counts['spam'] has {len(word_counts['spam'])} elements (one per vocab word)\")\n",
    "print(\"First 10 spam word counts:\", word_counts['spam'][:10])\n",
    "print(\"First 10 ham word counts:\", word_counts['ham'][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "72914825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tracing through the word counting process:\n",
      "============================================================\n",
      "First 10 vocabulary words:\n",
      "  'win' -> index 0\n",
      "  'a' -> index 1\n",
      "  'free' -> index 2\n",
      "  'vacation' -> index 3\n",
      "  'now' -> index 4\n",
      "  'lottery' -> index 5\n",
      "  'winner' -> index 6\n",
      "  'claim' -> index 7\n",
      "  'call' -> index 8\n",
      "  'mom' -> index 9\n",
      "\n",
      "Processing training texts for spam vs ham:\n",
      "----------------------------------------\n",
      "\n",
      "Document 1: 'win a free vacation now' -> spam\n",
      "  Tokens: ['win', 'a', 'free', 'vacation', 'now']\n",
      "  Vector (first 10): [1, 1, 1, 1, 1, 0, 0, 0, 0, 0]\n",
      "    Word 'win' appears 1 time(s)\n",
      "    Word 'a' appears 1 time(s)\n",
      "    Word 'free' appears 1 time(s)\n",
      "    Word 'vacation' appears 1 time(s)\n",
      "    Word 'now' appears 1 time(s)\n",
      "\n",
      "Document 2: 'free lottery winner claim' -> spam\n",
      "  Tokens: ['free', 'lottery', 'winner', 'claim']\n",
      "  Vector (first 10): [0, 0, 1, 0, 0, 1, 1, 1, 0, 0]\n",
      "    Word 'free' appears 1 time(s)\n",
      "    Word 'lottery' appears 1 time(s)\n",
      "    Word 'winner' appears 1 time(s)\n",
      "    Word 'claim' appears 1 time(s)\n",
      "\n",
      "Document 3: 'call mom for dinner' -> ham\n",
      "  Tokens: ['call', 'mom', 'for', 'dinner']\n",
      "  Vector (first 10): [0, 0, 0, 0, 0, 0, 0, 0, 1, 1]\n",
      "    Word 'call' appears 1 time(s)\n",
      "    Word 'mom' appears 1 time(s)\n",
      "\n",
      "Document 4: 'schedule a meeting for tomorrow' -> ham\n",
      "  Tokens: ['schedule', 'a', 'meeting', 'for', 'tomorrow']\n",
      "  Vector (first 10): [0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "    Word 'a' appears 1 time(s)\n",
      "\n",
      "Document 5: 'winner winner free prize' -> spam\n",
      "  Tokens: ['winner', 'winner', 'free', 'prize']\n",
      "  Vector (first 10): [0, 0, 1, 0, 0, 0, 2, 0, 0, 0]\n",
      "    Word 'free' appears 1 time(s)\n",
      "    Word 'winner' appears 2 time(s)\n",
      "\n",
      "Document 6: 'are we still on for lunch' -> ham\n",
      "  Tokens: ['are', 'we', 'still', 'on', 'for', 'lunch']\n",
      "  Vector (first 10): [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "Final word counts by class:\n",
      "Spam class - first 10 word counts: [1, 1, 3, 1, 1, 1, 3, 1, 0, 0]\n",
      "Ham class  - first 10 word counts: [0, 1, 0, 0, 0, 0, 0, 0, 1, 1]\n",
      "\n",
      "What this means:\n",
      "- Total spam documents: 3\n",
      "- Total ham documents: 3\n",
      "- Word 'free' (index 2) appears 3 times in spam docs\n",
      "- Word 'free' (index 2) appears 0 times in ham docs\n"
     ]
    }
   ],
   "source": [
    "# Let's trace through the actual execution step by step\n",
    "print(\"Tracing through the word counting process:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Let's manually trace what happens\n",
    "vocab_items = list(vocab.items())[:10]  # First 10 words for demonstration\n",
    "print(\"First 10 vocabulary words:\")\n",
    "for word, idx in vocab_items:\n",
    "    print(f\"  '{word}' -> index {idx}\")\n",
    "\n",
    "print(f\"\\nProcessing training texts for spam vs ham:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Show what happens for each text\n",
    "for i, (text, label) in enumerate(zip(train_texts, train_labels)):\n",
    "    print(f\"\\nDocument {i+1}: '{text}' -> {label}\")\n",
    "    \n",
    "    # Get the word vector for this document\n",
    "    doc_vector = vectorize(text, vocab)\n",
    "    \n",
    "    # Show which words appear in this document\n",
    "    tokens = tokenize(text)\n",
    "    print(f\"  Tokens: {tokens}\")\n",
    "    \n",
    "    # Show the first few elements of the vector\n",
    "    print(f\"  Vector (first 10): {doc_vector[:10]}\")\n",
    "    \n",
    "    # Show which specific words got counted\n",
    "    for word, idx in vocab_items:\n",
    "        if doc_vector[idx] > 0:\n",
    "            print(f\"    Word '{word}' appears {doc_vector[idx]} time(s)\")\n",
    "\n",
    "print(f\"\\nFinal word counts by class:\")\n",
    "print(f\"Spam class - first 10 word counts: {word_counts['spam'][:10]}\")\n",
    "print(f\"Ham class  - first 10 word counts: {word_counts['ham'][:10]}\")\n",
    "\n",
    "print(f\"\\nWhat this means:\")\n",
    "print(f\"- Total spam documents: {class_counts['spam']}\")\n",
    "print(f\"- Total ham documents: {class_counts['ham']}\")\n",
    "print(f\"- Word 'free' (index 2) appears {word_counts['spam'][2]} times in spam docs\")\n",
    "print(f\"- Word 'free' (index 2) appears {word_counts['ham'][2]} times in ham docs\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
